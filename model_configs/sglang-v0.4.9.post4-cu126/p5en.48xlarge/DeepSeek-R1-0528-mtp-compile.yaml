deployment:
  docker_image: "lmsysorg/sglang:v0.4.9.post4-cu126"
  container_name: "sglang"
  port: 8080
  
  # Universal Docker parameters
  docker_params:
    gpus: "all"
    shm-size: "747g"
    ipc: "host"
    network: "host"
    volume:
      - "/efs/200005/.cache/huggingface/hub:/root/.cache/huggingface/hub"
      - "/opt/dlami/nvme/:/sgl-workspace/sglang/model"
    env: 
      - "TORCHINDUCTOR_CACHE_DIR=/sgl-workspace/sglang/model/torchinductor_cache/deepseek-r1-0528/"
  command: "python3 -m sglang.launch_server"
  # Universal application arguments
  app_args:
    host: "0.0.0.0"
    model-path: "model/deepseek-ai/DeepSeek-R1-0528"
    trust-remote-code: true
    speculative-algorithm: "EAGLE"
    enable-torch-compile: true
    torch-compile-max-bs: 8
    speculative-num-steps: 1 
    speculative-eagle-topk: 1 
    tp-size: 8
    mem-fraction-static: 0.90
    tool-call-parser: "deepseekv3"
    reasoning-parser: "deepseek-r1"

test_matrix:
  input_tokens: [100, 1600, 6400, 12800]
  output_tokens: [100, 400, 1000]
  processing_num: [1, 4, 16, 32, 64, 128]
  random_tokens: [100, 1600, 6400, 12800]

test_config:
  requests_per_process: 5
  warmup_requests: 1
  cooldown_seconds: 5