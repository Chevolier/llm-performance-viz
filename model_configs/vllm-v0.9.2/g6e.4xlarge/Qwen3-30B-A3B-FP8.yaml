deployment:
  docker_image: "vllm/vllm-openai:v0.9.2"
  container_name: "vllm-qwen3-30b"
  port: 8080
  gpu_config:
    gpus: "all"
    gpu_memory_utilization: 0.95
    shm_size: "747g"
  model_config:
    model: "Qwen/Qwen3-30B-A3B-FP8"
    max_model_len: 16384
    trust_remote_code: true
    enable_reasoning: true
    tool_call_parser: "hermes"
    reasoning_parser: "deepseek_r1"
  volumes:
    - "/efs/200005/.cache/huggingface/hub:/root/.cache/huggingface/hub"

test_matrix:
  input_tokens: [100, 400, 1600, 6400, 12800]
  output_tokens: [20, 100, 400, 1000]
  processing_num: [1, 4, 16, 32]
  random_tokens: [100, 400, 1600, 6400, 12800]

test_config:
  requests_per_process: 5
  warmup_requests: 1
  cooldown_seconds: 5