deployment:
  docker_image: "vllm/vllm-openai:v0.9.2"
  container_name: "vllm"
  port: 8080
  
  # Universal Docker parameters
  docker_params:
    gpus: "all"
    shm-size: "747g"
    ipc: "host"
    volume:
      - "/efs/200005/.cache/huggingface/hub:/root/.cache/huggingface/hub"
  
  # Universal application arguments
  app_args:
    model: "Qwen/Qwen3-30B-A3B-FP8"
    gpu-memory-utilization: 0.95
    max-model-len: 16384
    trust-remote-code: true
    enable-reasoning: true
    tool-call-parser: "hermes"
    enable-auto-tool-choice: true
    reasoning-parser: "deepseek_r1"

test_matrix:
  input_tokens: [100, 400, 1600, 6400, 12800]
  output_tokens: [20, 100, 400, 1000]
  processing_num: [1, 4, 16, 32]
  random_tokens: [100, 400, 1600, 6400, 12800]

test_config:
  requests_per_process: 5
  warmup_requests: 1
  cooldown_seconds: 5